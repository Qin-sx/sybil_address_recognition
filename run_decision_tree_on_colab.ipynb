{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YM07UAkkM_v"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Qin-sx/sybil_address_recognition.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gMzq-D_XkT3H"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wT8kptstklo3"
      },
      "outputs": [],
      "source": [
        "!mv /content/sybil_address_recognition /content/sybil_demo_pipeline\n",
        "import os\n",
        "os.chdir('/content/sybil_demo_pipeline')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wq9nz57Okntj"
      },
      "outputs": [],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMB2OmaWk2Ws"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/contest/wallet_risk_score.zip -d /content/sybil_demo_pipeline/data/wallet_risk_score\n",
        "!mkdir /content/sybil_demo_pipeline/data/raw_data\n",
        "!mv /content/sybil_demo_pipeline/data/wallet_risk_score/*  /content/sybil_demo_pipeline/data/raw_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeZQHZvDryS5"
      },
      "outputs": [],
      "source": [
        "!mkdir data/features\n",
        "!mv /content/sybil_demo_pipeline/data/raw_data/train_addresses.parquet /content/sybil_demo_pipeline/data/raw_data/train_dataset.parquet\n",
        "!mv /content/sybil_demo_pipeline/data/raw_data/test_addresses.parquet /content/sybil_demo_pipeline/data/raw_data/test_dataset.parquet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def get_all_candidates(file_path):\n",
        "    train_datafile = os.path.join(file_path, \"train_dataset.parquet\")\n",
        "    test_datafile = os.path.join(file_path, \"test_dataset.parquet\")\n",
        "    train_df = pd.read_parquet(train_datafile)\n",
        "    test_df = pd.read_parquet(test_datafile)\n",
        "    train_addresses = train_df['ADDRESS']\n",
        "    test_addresses = test_df['ADDRESS']\n",
        "    all_addresses = pd.concat([train_addresses, test_addresses], ignore_index=True)\n",
        "    all_addresses_df = pd.DataFrame(all_addresses, columns=['ADDRESS'])\n",
        "    return all_addresses_df\n",
        "\n",
        "def get_transaction_partners(transactions_df, address_df, columns=['FROM_ADDRESS', 'TO_ADDRESS']):\n",
        "    # Initialize a dictionary to store the transaction partners for each address\n",
        "    address_partners = {address: set() for address in address_df['ADDRESS']}\n",
        "\n",
        "    # Iterate over each row in the transactions_df\n",
        "    for _, row in transactions_df.iterrows():\n",
        "        from_address = row[columns[0]]\n",
        "        to_address = row[columns[1]]\n",
        "\n",
        "        # Add the to_address to the from_address's partner set and vice versa\n",
        "        if from_address in address_partners:\n",
        "            address_partners[from_address].add(to_address)\n",
        "        if to_address in address_partners:\n",
        "            address_partners[to_address].add(from_address)\n",
        "\n",
        "    # Convert the dictionary to a DataFrame\n",
        "    result_df = pd.DataFrame({\n",
        "        'ADDRESS': list(address_partners.keys()),\n",
        "        'PARTNERS': [list(partners) for partners in address_partners.values()]\n",
        "    })\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def save_unique_partners(transaction_partners_df, address_df, output_path):\n",
        "    # Extract all partners\n",
        "    all_partners = set()\n",
        "    for partners in transaction_partners_df['PARTNERS']:\n",
        "        if partners is not None:  # Check for NULL values\n",
        "            all_partners.update(partners)\n",
        "\n",
        "    # Remove addresses that are in address_df['ADDRESS']\n",
        "    address_set = set(address_df['ADDRESS'])\n",
        "    unique_partners = all_partners - address_set\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    unique_partners_df = pd.DataFrame(list(unique_partners), columns=['ADDRESS'])\n",
        "\n",
        "    # Remove rows with NULL values in ADDRESS column\n",
        "    unique_partners_df.dropna(subset=['ADDRESS'], inplace=True)\n",
        "\n",
        "    # Merge PARTNERS from transaction_partners_df based on ADDRESS\n",
        "    merged_df = unique_partners_df.merge(transaction_partners_df[['ADDRESS', 'PARTNERS']], on='ADDRESS', how='left')\n",
        "\n",
        "    # Save to parquet file\n",
        "    output_file = os.path.join(output_path, \"partners.parquet\")\n",
        "    merged_df.to_parquet(output_file, index=False)\n",
        "\n",
        "def main(file_path, output_path):\n",
        "    # Get all addresses\n",
        "    address_df = get_all_candidates(file_path)\n",
        "\n",
        "    # Read the transactions data\n",
        "    transactions_datafile = os.path.join(file_path, \"transactions.parquet\")\n",
        "    transactions_df = pd.read_parquet(transactions_datafile)\n",
        "\n",
        "    # Get transaction partners\n",
        "    transaction_partners_df = get_transaction_partners(transactions_df, address_df, columns=['FROM_ADDRESS', 'TO_ADDRESS'])\n",
        "\n",
        "    # Save the transaction partners to a .parquet file\n",
        "    output_file = os.path.join(output_path, \"transaction_partners.parquet\")\n",
        "    transaction_partners_df.to_parquet(output_file, index=False)\n",
        "\n",
        "    # Save unique partners to a separate .parquet file\n",
        "    save_unique_partners(transaction_partners_df, address_df, output_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"./data/raw_data/\"\n",
        "    output_path = \"./data/features/\"\n",
        "    main(file_path, output_path)"
      ],
      "metadata": {
        "id": "LBvP_fqQ4W7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 指定文件夹路径\n",
        "folder_path = '/content/sybil_demo_pipeline/data/raw_data/token_transfers'\n",
        "\n",
        "# 加载 partners.parquet 文件\n",
        "partners_path = '/content/sybil_demo_pipeline/data/features/partners.parquet'  # 假设 partners.parquet 文件路径\n",
        "partners_df = pd.read_parquet(partners_path)\n",
        "\n",
        "# 获取 partners.parquet 中的 ADDRESS 列\n",
        "partners_addresses = partners_df['ADDRESS'].dropna().unique()\n",
        "\n",
        "# 获取所有 token_transfers 的 parquet 文件\n",
        "files = [f for f in os.listdir(folder_path) if f.startswith('token_transfers.parquet')]\n",
        "\n",
        "# 读取并合并文件，同时进行过滤\n",
        "df_list = []\n",
        "for file in files:\n",
        "    file_path = os.path.join(folder_path, file)\n",
        "    df = pd.read_parquet(file_path)\n",
        "    # 过滤出只有地址在 partners_addresses 中的记录\n",
        "    filtered_df = df[\n",
        "        df['FROM_ADDRESS'].isin(partners_addresses) |\n",
        "        df['TO_ADDRESS'].isin(partners_addresses) |\n",
        "        df['ORIGIN_FROM_ADDRESS'].isin(partners_addresses) |\n",
        "        df['ORIGIN_TO_ADDRESS'].isin(partners_addresses)\n",
        "    ]\n",
        "    if not filtered_df.empty:\n",
        "        df_list.append(filtered_df)\n",
        "\n",
        "# 合并过滤后的数据\n",
        "if df_list:\n",
        "    merged_df = pd.concat(df_list, ignore_index=True)\n",
        "    # 保存合并后的文件\n",
        "    merged_df.to_parquet('/content/sybil_demo_pipeline/data/raw_data/token_transfers.parquet', index=False)\n",
        "else:\n",
        "    print(\"No data to save after filtering.\")"
      ],
      "metadata": {
        "id": "NYG2Ej8E2qCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXBkmDRKdbSE"
      },
      "outputs": [],
      "source": [
        "!mkdir ./saved_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python feature_process_2.py"
      ],
      "metadata": {
        "id": "5rCPNNi93rZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/sybil_demo_pipeline/data/features/transactions_feature_partner.parquet /content/sybil_demo_pipeline/data/features/transactions_feature.parquet"
      ],
      "metadata": {
        "id": "7A71PW876dmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxJRlavODVls"
      },
      "outputs": [],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe8cbBSd67ah"
      },
      "outputs": [],
      "source": [
        "!python train2.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXJ19rpV7o4s"
      },
      "outputs": [],
      "source": [
        "!python inference2.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as4Px8skYY4O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYUGhBBQZdBS"
      },
      "outputs": [],
      "source": [
        "!zip -r sybil_demo_pipeline.zip ./sybil_demo_pipeline -x \"./sybil_demo_pipeline/data/*\" -x \"./sybil_demo_pipeline/saved_model/*\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}