{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reference: https://github.com/FighterLYL/GraphNeuralNetwork"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YM07UAkkM_v",
        "outputId": "f29140f6-fe0d-40a5-ff9c-8debbc1216fe"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Qin-sx/sybil_demo_pipeline.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMzq-D_XkT3H",
        "outputId": "a753187d-140d-4319-b837-2f4a9e5619bf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wT8kptstklo3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/sybil_demo_pipeline')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wq9nz57Okntj"
      },
      "outputs": [],
      "source": [
        "!mkdir data\n",
        "!mkdir ./saved_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMB2OmaWk2Ws",
        "outputId": "b3282b91-c90e-4880-e475-3bffbcb01e38"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/contest/sybil_address_prediction.zip -d /content/sybil_demo_pipeline/data/\n",
        "!mkdir /content/sybil_demo_pipeline/data/raw_data\n",
        "!mv /content/sybil_demo_pipeline/data/sybil_address_prediction/*  /content/sybil_demo_pipeline/data/raw_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeZQHZvDryS5"
      },
      "outputs": [],
      "source": [
        "!mkdir data/features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOkhUfW231qC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def get_all_candidates(file_path):\n",
        "    train_datafile = os.path.join(file_path, \"train_dataset.parquet\")\n",
        "    test_datafile = os.path.join(file_path, \"test_dataset.parquet\")\n",
        "    train_df = pd.read_parquet(train_datafile)\n",
        "    test_df = pd.read_parquet(test_datafile)\n",
        "    train_addresses = train_df['ADDRESS']\n",
        "    test_addresses = test_df['ADDRESS']\n",
        "    all_addresses = pd.concat([train_addresses, test_addresses], ignore_index=True)\n",
        "    all_addresses_df = pd.DataFrame(all_addresses, columns=['ADDRESS'])\n",
        "    return all_addresses_df\n",
        "\n",
        "def get_transaction_partners(transactions_df, address_df, columns=['FROM_ADDRESS', 'TO_ADDRESS']):\n",
        "    # Initialize a dictionary to store the transaction partners for each address\n",
        "    address_partners = {address: set() for address in address_df['ADDRESS']}\n",
        "\n",
        "    # Iterate over each row in the transactions_df\n",
        "    for _, row in transactions_df.iterrows():\n",
        "        from_address = row[columns[0]]\n",
        "        to_address = row[columns[1]]\n",
        "\n",
        "        # Add the to_address to the from_address's partner set and vice versa\n",
        "        if from_address in address_partners:\n",
        "            address_partners[from_address].add(to_address)\n",
        "        if to_address in address_partners:\n",
        "            address_partners[to_address].add(from_address)\n",
        "\n",
        "    # Convert the dictionary to a DataFrame\n",
        "    result_df = pd.DataFrame({\n",
        "        'ADDRESS': list(address_partners.keys()),\n",
        "        'PARTNERS': [list(partners) for partners in address_partners.values()]\n",
        "    })\n",
        "\n",
        "    return result_df\n",
        "\n",
        "def save_unique_partners(transaction_partners_df, address_df, output_path):\n",
        "    # Extract all partners\n",
        "    all_partners = set()\n",
        "    for partners in transaction_partners_df['PARTNERS']:\n",
        "        if partners is not None:  # Check for NULL values\n",
        "            all_partners.update(partners)\n",
        "\n",
        "    # Remove addresses that are in address_df['ADDRESS']\n",
        "    address_set = set(address_df['ADDRESS'])\n",
        "    unique_partners = all_partners - address_set\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    unique_partners_df = pd.DataFrame(list(unique_partners), columns=['ADDRESS'])\n",
        "\n",
        "    # Remove rows with NULL values in ADDRESS column\n",
        "    unique_partners_df.dropna(subset=['ADDRESS'], inplace=True)\n",
        "\n",
        "    # Merge PARTNERS from transaction_partners_df based on ADDRESS\n",
        "    merged_df = unique_partners_df.merge(transaction_partners_df[['ADDRESS', 'PARTNERS']], on='ADDRESS', how='left')\n",
        "\n",
        "    # Save to parquet file\n",
        "    output_file = os.path.join(output_path, \"partners.parquet\")\n",
        "    merged_df.to_parquet(output_file, index=False)\n",
        "\n",
        "def main(file_path, output_path):\n",
        "    # Get all addresses\n",
        "    address_df = get_all_candidates(file_path)\n",
        "\n",
        "    # Read the transactions data\n",
        "    transactions_datafile = os.path.join(file_path, \"transactions.parquet\")\n",
        "    transactions_df = pd.read_parquet(transactions_datafile)\n",
        "\n",
        "    # Get transaction partners\n",
        "    transaction_partners_df = get_transaction_partners(transactions_df, address_df, columns=['FROM_ADDRESS', 'TO_ADDRESS'])\n",
        "\n",
        "    # Save the transaction partners to a .parquet file\n",
        "    output_file = os.path.join(output_path, \"transaction_partners.parquet\")\n",
        "    transaction_partners_df.to_parquet(output_file, index=False)\n",
        "\n",
        "    # Save unique partners to a separate .parquet file\n",
        "    save_unique_partners(transaction_partners_df, address_df, output_path)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"./data/raw_data/\"\n",
        "    output_path = \"./data/features/\"\n",
        "    main(file_path, output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9_L0sI0VTP7",
        "outputId": "c2057f61-117f-4863-ca8b-6bef096b18f0"
      },
      "outputs": [],
      "source": [
        "!python feature_process_2.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCTJv8BfJBoo"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    file_path = \"./data/features/transactions_feature_partner.parquet\"\n",
        "    df = pd.read_parquet(file_path)\n",
        "    addresses = df['ADDRESS']\n",
        "    addresses_df = pd.DataFrame(addresses, columns=['ADDRESS'])\n",
        "\n",
        "    transactions_datafile = \"./data/raw_data/transactions.parquet\"\n",
        "    transactions_df = pd.read_parquet(transactions_datafile)\n",
        "\n",
        "    transaction_partners_df = get_transaction_partners(transactions_df, addresses_df, columns=['FROM_ADDRESS', 'TO_ADDRESS'])\n",
        "\n",
        "    output_path = \"./data/features/matched_partners.parquet\"\n",
        "    transaction_partners_df.to_parquet(output_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmBh-dSzBT0N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import itertools\n",
        "\n",
        "def build_adjacency(adj_dict, address_count):\n",
        "    \"\"\"Create adjacency matrix from adjacency list\"\"\"\n",
        "    edge_index = []\n",
        "    # num_nodes = len(adj_dict)\n",
        "    for src, dst in adj_dict.items():\n",
        "        edge_index.extend([src, v] for v in dst)\n",
        "        edge_index.extend([v, src] for v in dst)\n",
        "    # Remove duplicate edges\n",
        "    edge_index = list(k for k, _ in itertools.groupby(sorted(edge_index)))\n",
        "    edge_index = np.asarray(edge_index)\n",
        "    adjacency = sp.coo_matrix((np.ones(len(edge_index)),\n",
        "                               (edge_index[:, 0], edge_index[:, 1])),\n",
        "                shape=(address_count, address_count), dtype=\"float32\")\n",
        "    return adjacency\n",
        "\n",
        "def create_adjacency_matrix(file_path):\n",
        "    # Read transaction_partners.parquet file\n",
        "    transaction_partners_df = pd.read_parquet(file_path)\n",
        "\n",
        "    # Build adjacency list\n",
        "    adj_dict = {}\n",
        "    address_to_index = {address: idx for idx, address in enumerate(transaction_partners_df['ADDRESS'])}\n",
        "    address_count = len(transaction_partners_df['ADDRESS'])\n",
        "\n",
        "    # print(\"Address to Index Mapping:\")\n",
        "    # for address, idx in address_to_index.items():\n",
        "    #     print(f\"{address}: {idx}\")\n",
        "\n",
        "    for _, row in transaction_partners_df.iterrows():\n",
        "        address = row['ADDRESS']\n",
        "        partners = row['PARTNERS']\n",
        "\n",
        "        adj_dict[address_to_index[address]] = []\n",
        "\n",
        "        # Check if partners is None or NaN\n",
        "        if partners is None:\n",
        "            continue\n",
        "\n",
        "        # Filter out missing partners\n",
        "        valid_partners = [partner for partner in partners if partner in address_to_index]\n",
        "\n",
        "        # Add valid partners to adjacency list\n",
        "        adj_dict[address_to_index[address]].extend([address_to_index[partner] for partner in valid_partners])\n",
        "\n",
        "    # Create adjacency matrix\n",
        "    adjacency_matrix = build_adjacency(adj_dict, address_count)\n",
        "\n",
        "    return adjacency_matrix\n",
        "\n",
        "file_path = \"./data/features/matched_partners.parquet\"\n",
        "adjacency_matrix = create_adjacency_matrix(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c29JYNQVHfdh"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import pickle\n",
        "from collections import namedtuple\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "\n",
        "def tensor_from_numpy(x, device):\n",
        "    return torch.from_numpy(x).to(device)\n",
        "\n",
        "def normalization(adjacency):\n",
        "    \"\"\"calculate L=D^-0.5 * (A+I) * D^-0.5\"\"\"\n",
        "    adjacency += sp.eye(adjacency.shape[0])    # Increased self-connection\n",
        "    degree = np.array(adjacency.sum(1))\n",
        "    d_hat = sp.diags(np.power(degree, -0.5).flatten())\n",
        "    return d_hat.dot(adjacency).dot(d_hat).tocoo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfRE4_fnTTI-",
        "outputId": "324bbb3d-b0c6-4b2b-efc8-f329d92552d6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "feature_path = \"./data/features/transactions_feature_partner.parquet\"\n",
        "feature_df = pd.read_parquet(feature_path)\n",
        "feature = feature_df.drop(['ADDRESS'], axis=1)\n",
        "\n",
        "label_path = \"./data/raw_data/train_dataset.parquet\"\n",
        "label_df = pd.read_parquet(label_path)\n",
        "\n",
        "label = label_df['LABEL'].astype(int)\n",
        "\n",
        "label_num = len(label_df)\n",
        "\n",
        "test_path = \"./data/raw_data/test_dataset.parquet\"\n",
        "test__df = pd.read_parquet(test_path)\n",
        "test_num = len(test__df)\n",
        "\n",
        "# Generate ID list\n",
        "ids = list(range(label_num))\n",
        "train_ids, valid_ids = train_test_split(ids, test_size=0.2, random_state=42)\n",
        "test_ids = np.arange(label_num, label_num+test_num)\n",
        "\n",
        "#Load data and convert to torch.Tensor\n",
        "feature = feature.to_numpy()\n",
        "# node_feature = feature / feature.sum(1, keepdims=True)  # Normalize data so that each row sums to 1\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "node_feature = scaler.fit_transform(feature)\n",
        "\n",
        "tensor_x = tensor_from_numpy(node_feature, DEVICE).float()\n",
        "tensor_y = tensor_from_numpy(label.to_numpy(), DEVICE)\n",
        "tensor_train_mask = tensor_from_numpy(np.array(train_ids), DEVICE)\n",
        "tensor_val_mask = tensor_from_numpy(np.array(valid_ids), DEVICE)\n",
        "tensor_test_mask = tensor_from_numpy(np.array(test_ids), DEVICE)\n",
        "normalize_adjacency = normalization(adjacency_matrix) \n",
        "# print(\"node_feature.shape\",node_feature.shape)\n",
        "num_nodes, input_dim = node_feature.shape\n",
        "indices = torch.from_numpy(np.asarray([normalize_adjacency.row,\n",
        "                                       normalize_adjacency.col]).astype('int64')).long()\n",
        "values = torch.from_numpy(normalize_adjacency.data.astype(np.float32))\n",
        "tensor_adjacency = torch.sparse_coo_tensor(indices, values,(num_nodes, num_nodes)).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJPXKgscJ95c"
      },
      "outputs": [],
      "source": [
        "class GraphConvolution(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, use_bias=True):\n",
        "        \"\"\"Graph Convolution: L*X*\\theta\n",
        "\n",
        "        Args:\n",
        "        ----------\n",
        "            input_dim: int\n",
        "                Dimension of input features for nodes\n",
        "            output_dim: int\n",
        "                Dimension of output features\n",
        "            use_bias : bool, optional\n",
        "                Whether to use bias\n",
        "        \"\"\"\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.use_bias = use_bias\n",
        "        self.weight = nn.Parameter(torch.Tensor(input_dim, output_dim))\n",
        "        if self.use_bias:\n",
        "            self.bias = nn.Parameter(torch.Tensor(output_dim))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        init.kaiming_uniform_(self.weight)\n",
        "        if self.use_bias:\n",
        "            init.zeros_(self.bias)\n",
        "\n",
        "    def forward(self, adjacency, input_feature):\n",
        "        \"\"\"The adjacency matrix is a sparse matrix, so sparse matrix multiplication is used in computation\n",
        "\n",
        "        Args:\n",
        "        -------\n",
        "            adjacency: torch.sparse.FloatTensor\n",
        "                Adjacency matrix\n",
        "            input_feature: torch.Tensor\n",
        "                Input features\n",
        "        \"\"\"\n",
        "        support = torch.mm(input_feature, self.weight)\n",
        "        output = torch.sparse.mm(adjacency, support)\n",
        "        if self.use_bias:\n",
        "            output += self.bias\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "            + str(self.input_dim) + ' -> ' \\\n",
        "            + str(self.output_dim) + ')'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_zx2RudJ9vn"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GcnNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Define a model containing four layers of GraphConvolution\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, dropout_rate=0.1):\n",
        "        super(GcnNet, self).__init__()\n",
        "        self.gcn1 = GraphConvolution(input_dim, 64)\n",
        "        self.gcn2 = GraphConvolution(64, 32)       \n",
        "        self.gcn3 = GraphConvolution(32, 16)       \n",
        "        self.gcn4 = GraphConvolution(16, 2)        \n",
        "        self.dropout = nn.Dropout(dropout_rate)    \n",
        "\n",
        "    def forward(self, adjacency, feature):\n",
        "        h = F.relu(self.gcn1(adjacency, feature))\n",
        "        h = self.dropout(h)\n",
        "        h = F.relu(self.gcn2(adjacency, h))\n",
        "        h = self.dropout(h)\n",
        "        h = F.relu(self.gcn3(adjacency, h))\n",
        "        h = self.dropout(h)\n",
        "        logits = self.gcn4(adjacency, h)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVsGGhvMKFEf"
      },
      "outputs": [],
      "source": [
        "LEARNING_RATE = 0.001\n",
        "# LEARNING_RATE = 0.05\n",
        "WEIGHT_DECAY = 5e-4\n",
        "EPOCHS = 2000\n",
        "\n",
        "# model definitionï¼šModel, Loss, Optimizer\n",
        "model = GcnNet(input_dim).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       lr=LEARNING_RATE,\n",
        "                       weight_decay=WEIGHT_DECAY)\n",
        "# optimizer = optim.SGD(model.parameters(),\n",
        "#             lr=LEARNING_RATE,\n",
        "#             momentum=0.9,\n",
        "#             weight_decay=WEIGHT_DECAY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Yt6eoATLl5T"
      },
      "outputs": [],
      "source": [
        "def test(mask):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(tensor_adjacency, tensor_x)\n",
        "        test_mask_logits = logits[mask]\n",
        "        predict_y = test_mask_logits.max(1)[1]\n",
        "        accuarcy = torch.eq(predict_y, tensor_y[mask]).float().mean()\n",
        "    return accuarcy, test_mask_logits.cpu().numpy(), tensor_y[mask].cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cKUXay-KIs7"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    loss_history = []\n",
        "    val_acc_history = []\n",
        "    model.train()\n",
        "    train_y = tensor_y[tensor_train_mask]\n",
        "\n",
        "    val_best = 0\n",
        "    best_model_path = \"best_model.pth\"\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        logits = model(tensor_adjacency, tensor_x)  # Forward propagation\n",
        "        train_mask_logits = logits[tensor_train_mask]   # Only select training nodes for supervision\n",
        "        loss = criterion(train_mask_logits, train_y)    # Calculate loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()     # Backward propagation to compute gradients\n",
        "        optimizer.step()    # Use optimizer to update gradients\n",
        "        train_acc, _, _ = test(tensor_train_mask)     # Calculate training accuracy\n",
        "        val_acc, _, _ = test(tensor_val_mask)     # Calculate validation accuracy\n",
        "        if val_acc.item() > val_best:\n",
        "          val_best = val_acc.item()\n",
        "          print(\"new val\", val_best)\n",
        "          torch.save(model.state_dict(), best_model_path)\n",
        "        # Record loss and accuracy during training for plotting\n",
        "        loss_history.append(loss.item())\n",
        "        val_acc_history.append(val_acc.item())\n",
        "        print(\"Epoch {:03d}: Loss {:.4f}, TrainAcc {:.4}, ValAcc {:.4f}\".format(\n",
        "            epoch, loss.item(), train_acc.item(), val_acc.item()))\n",
        "\n",
        "    print(\"val_best\", val_best)\n",
        "    return loss_history, val_acc_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y_H_wIuKLfo",
        "outputId": "26f041c5-988b-4794-db1d-1fc4c5d565a6"
      },
      "outputs": [],
      "source": [
        "loss, val_acc = train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wovvnf7duRAS",
        "outputId": "2b081e72-1372-4f5d-92c5-3788b6d10d35"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define inference function\n",
        "def inference():\n",
        "    # Load the best model\n",
        "    best_model_path = \"best_model.pth\"\n",
        "    model.load_state_dict(torch.load(best_model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # Perform inference on the test set\n",
        "    with torch.no_grad():\n",
        "        logits = model(tensor_adjacency, tensor_x)\n",
        "        test_mask_logits = logits[tensor_test_mask]\n",
        "        predict_y = test_mask_logits.max(1)[1]\n",
        "\n",
        "\n",
        "    # Save prediction results to a CSV file\n",
        "    test_addresses = feature_df['ADDRESS'].iloc[tensor_test_mask.cpu().numpy()]\n",
        "    test_df = pd.DataFrame({\n",
        "        'ADDRESS': test_addresses,\n",
        "        'PRED': predict_y.cpu().numpy()\n",
        "    })\n",
        "    test_df.to_csv(\"pred.csv\", index=False)\n",
        "\n",
        "# Call the inference function\n",
        "inference()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryiVi_6Nfbbh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS2YWmTOfe1U",
        "outputId": "31af4e1e-d9c5-4faf-d378-e6d9c55fb767"
      },
      "outputs": [],
      "source": [
        "!zip -r sybil_demo_pipeline.zip /content/sybil_demo_pipeline -x '/content/sybil_demo_pipeline/data/*' -x '/content/sybil_demo_pipeline/saved_model/*'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
